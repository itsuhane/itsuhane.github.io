---
layout: post
title: "最小二乘问题（六）"
date: 2016-12-26 09:01:05 GMT
tags:
- least squares
comment: true
---

前面提到的最小二乘有一个共同的假设，就是残差向量遵循高斯分布。

那么，如果不满足高斯分布假设呢？

通常我们模型的噪音都是符合高斯分布的，对于模型中蕴含的非高斯分布情形，我们不在这里讨论了。我们介绍一种在高斯假设下依旧会引入非高斯分布残差的情形，并介绍在最小二乘中处理它的一种手段。

通常我们在建模时认为数据点都是由模型产生并带有一定的噪音。但在真实中，数据可能掺杂着不属于当前模型的点，叫做外点（outlier）。这些外点会污染数据，表现上便是出现了不符合高斯分布的噪音。由于我们采用平方项，当与模型偏差很远的点存在时，它会对模型的估计带来严重的不良影响。

为了减小这种影响，我们可以采用加权的最小二乘，即

$$
\min \sum w_i\|r_i\|^2.
$$

理想情况下，只要令外点对应的权值为 0 即可从优化中忽略掉它们。

然而真实情况中，我们可能不知道外点都有哪些，因此我们需要一个可以自动降低外点影响的加权方案。"M-估计"便是一种常用的方法，它在优化中引入了“鲁棒函数”来根据误差的大小自动调整权重：

$$
\min \sum \rho_i(\|r_i\|).
$$

这里的 $\rho_i$ 便是鲁棒函数，如果 $\rho_i(x) = x^2$ ，我们就得到了一般的最小二乘问题。为了减小外点（带有巨大误差的点）的影响，一般要取 $\lim_{x\to\infty}\frac{\rho(x)}{x^2} \to 0$ 的函数。也就是当误差越大，它对整体优化的影响就越弱于普通的平方项。但同时，$\rho(x)$ 还应该满足从 0 向两个方向增长，否则优化可能会收敛到错误的结果（比如直接拒绝掉全部的数据点从而得到更小的能量）。

常见的鲁棒函数选择很多，比如 Huber 函数、Cauchy 函数等等。网上资料很丰富，这里就不列举了。

加入了鲁棒函数后，我们的问题就不是最小二乘了。但前面非线性最小二乘中，我们通过线性化得到了线性最小二乘子问题。这里我们也尝试一下转化它成为我们熟悉的形式。

我们比较带鲁棒函数的能量和一般的最小二乘能量的导数，根据极值的一阶条件我们知道：

$$
\begin{aligned}
\frac{\partial \|r\|^2}{\partial x} &= 2r^T\frac{\partial r}{\partial x} = 0 \\\
\frac{\partial \rho(\|r\|)}{\partial x} &= \frac{d\rho}{dx} \frac{1}{\|r\|}r^T\frac{\partial r}{\partial x} = 0.
\end{aligned}
$$

如果我们记 $w(x) = \frac{d\rho}{dx}\frac{1}{x}$，进一步地令常系数 $w’= w(\\|r\\|)$ ，然后构造加权的最小二乘优化 $\min \frac{w’}{2}\\|r\\|^2$，它的 Jacobian 是：

$$
\frac12\frac{\partial w’\|r\|^2}{\partial r} = w’r^T\nabla r.
$$

可以发现，它与前面带有鲁棒函数的最小二乘优化形式是相同的。

也就是说，我们可以在每一轮迭代时，采用当前的 $\\|r\\|$ 计算加权值。固定加权值后，将当前迭代中的优化问题看做一个加权最小二乘问题进行优化。这时，带鲁棒函数的 M-估计与带更新的加权最小二乘是等价的。