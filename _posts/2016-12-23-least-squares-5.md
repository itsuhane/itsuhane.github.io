---
layout: post
title: "最小二乘问题（五）"
date: 2016-12-23 09:00:59 GMT
tags:
- least squares
comment: true
---

上回末有人搞事儿，希望加入非高斯的噪声……但我们偏不！我们要先看一些更重要的扩展。

我们记 $r(x) = Ax-b$ ，这是我们的（线性）残差函数。那么标准的最小二乘问题就是 $\min \\|r(x)\\|^2$ 。残差函数关于变量 $x$ 的导数（Jacobian）记作 $J_r$ ，我们知道 $J_r = A$，于是最小二乘的标准方程还可以写成 $J_r^TJ_rx = J_r^Tb$ 。

如果 $r(x)$ 并非关于 $x$ 的线性函数呢？此时我们得到了年轻人的第一个非线性最小二乘问题！

它与线性最小二乘最大的区别便在于：非线性最小二乘通常不是凸优化问题。这是什么意思？恩我们不在这里解释了。那这个区别有什么用？恩它没什么用，它是来给我们制造新的困难的，这个困难就是：对于线性最小二乘问题，你可以求解标准方程得到全局最优解；然而对于非线性最小二乘问题……

* 它有标准方程么？是什么形式？
* 它的解如何寻找？可以求解标准方程么？
* 它有全局最优解么？

恩……对于一般的非线性最小二乘问题，通常我们只能找到一个局部最优解，至于局部最优是不是全局最优，这就要靠手段加运气了。它的解要如何寻找呢？答案是从一个初始的猜测出发，一步步走到更优的解，直到收敛——这也叫做迭代优化方法。至于标准方程，恩我们下面来看看非线性最小二乘的标准方程。

如果残差函数有具体的表达式，那当然是可以更好地分析，不过这里我们从它的一般形式入手。一个问题……它不是线性的，怎么办？线性化咯！

于是我们将残差函数 $r(x)$ 用它的一阶近似表达，也就是

$$
r(x_0+\Delta x) = r(x_0)+J_r\Delta x.
$$

这样，当我们从一个初始值 $x_0$ 出发时，我们可以寻找一个让一阶近似更优的变化量 $\Delta x$，这样我们便“大约”将原始问题优化了一些。

这个思路写成最优化形式，就是

$$
\min_{\Delta x} \|J_r\Delta x + r(x_0)\|^2.
$$

它是个关于 $\Delta x$ 的线性最小二乘问题，套用我们前面的知识，我们知道它的标准方程是

$$
J_r^TJ_r\Delta x = -J_r^Tr(x_0).
$$

前面我们已经提到了，整个问题需要一步步不停地迭代，所以说这里我们只是完成了一个小目标，接下来我们要更新变量的值并进一步优化。

所以完整的非线性最小二乘优化算法是这样的：

1. $x \gets x_0;$
2. $\Delta x \gets \arg\min_{\Delta x} \|J_r\Delta x+r(x)\|^2;$
   * 求解标准方程 $J_r^TJ_r\Delta x = -J_r^Tr(x);$
3. $x \gets x+\gamma \Delta x;$
4. 回到 2 ，除非 $\|\Delta x\|<\epsilon$ 。

这个算法叫做 Gauss-Newton 算法。而其中关于 $\Delta x$ 的标准方程也叫作 Gauss-Newton 标准方程。$\gamma > 0 $ 是一个控制步长的系数。

那么如果有多项呢？如果残差遵循不同的高斯分布呢？线性化，然后回忆前面的故事吧！
