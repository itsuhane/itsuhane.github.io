---
layout: post
title: "最小二乘问题（十四）"
date: 2017-01-11 09:00:41 GMT
tags:
- least squares
comment: true
---

与固定变量相对的，我们还可以“忽略”某个变量。这里的忽略当然不是从我们的问题中去掉这个变量，而是允许它取任何可能的值，在它全体可能取值范围内求解最优的剩余变量来最小化我们的目标函数。

我们把这种“忽略”叫做边缘化（Marginalization）。在一个一般的非线性最小二乘中直接边缘化变量的情形比较复杂，因此这里我们只考虑在每一步线性子问题中边缘化掉某个梯度方向。

我们依旧套用上一篇中的标准方程。完整的系统中的标准方程包含了全体梯度方向。如果我们想要边缘化某个方向，不妨是 $\Delta x_1$ 。意味着我们想得到一个新的标准方程，在这个标准方程中不包含 $\Delta x_1$ 作为变量。

“可是上一篇中我们不是已经有了一个新的并且没有 $\Delta x_1$ 的标准方程了吗？”是这样没错，但回忆上一篇，我们得到的那个问题中，$x_1$ 变量被固定了，也就是说并不是 $\Delta x_1$ 被忽略了，而是它被固定为了 $\Delta x_1 = 0$ 。而这里我们所希望的是让它可以随心所欲，因此我们需要用另外一个技巧：高斯消元。

我们将原始的标准方程的矩阵重新分块，然后将标准方程写为下面这样：

$$
\begin{pmatrix}
J_1^TJ_1 & J_1^TJ_{\bar{1}} \\
J_{\bar{1}}^TJ_1 & J_{\bar{1}}^TJ_{\bar{1}}
\end{pmatrix}\begin{pmatrix}\Delta x_1 \\ \Delta x_{\bar{1}}\end{pmatrix}
= -\begin{pmatrix}J_1^T \\ J_{\bar{1}}^T\end{pmatrix}r.
$$

我们用 $\bar{1}$ 表示除去 1 之外的部分，$\Delta x_{\bar{1}}$ 便是我们想要保留的全部变量，其它部分也同样类比。我们对方程进行块状行变换，将方程左右的第一行（$x_1$ 对应的行）左侧乘上$-J_{\bar{1}}^TJ_1(J_1^TJ_1)^{-1}$ 后加到第二行上，于是第二行便成为了：

$$
(J_{\bar{1}}^TJ_{\bar{1}}- J_{\bar{1}}^TJ_1 (J_1^TJ_1)^{-1}   J_1^TJ_{\bar{1}}) \Delta x_{\bar{1}}
= -(J_{\bar{1}}^T - J_{\bar{1}}^TJ_1 (J_1^TJ_1)^{-1} J_1^T)r.
$$

这个形式看起来复杂，但是左侧的系数矩阵其实正是原先完整的系数矩阵关于 $J_1^TJ_1$ 部分的 Schur 补。这一操作我们在前面的文章中介绍过。在这个新的方程中，我们的变量不再包含 $\Delta x_1$ ，但可以发现无论是左侧的系数还是右侧的余项中，都包含了来自 $\Delta x_1$ 的梯度信息 $J_1$ 。正是因为我们加入了这部分信息，才允许我们可以“忽略”掉对应的变量直接求解。

如果我们求解了这第二行的部分，接下来是可以将 $\Delta x_{\bar{1}}$ 代入回第一行继续解出对应当前最优的 $\Delta x_1$ 的。不过如果我们强行让 $\Delta x_{\bar{1}} = 0$ 并代入，得到了什么？没错，这时我们就得到了上一篇文章中描述的固定变量情形了。

进一步的，我们可以给 $\Delta x_{\bar{1}}$ 赋予任意需要的值，然后利用第一行求解对应的 $\Delta x_1$ ，这实际上对应了给定前者作为条件时后者的最大似然估计。这一系列的概率背景就不在这里展开介绍了，有机会在另外的文字里描述一下。
